{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c09c76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hadoop.conf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     24\u001b[0m config_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhadoop.conf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 25\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mread_hadoop_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m display_core_components(config)\n",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m, in \u001b[0;36mread_hadoop_config\u001b[1;34m(config_file)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_hadoop_config\u001b[39m(config_file):\n\u001b[0;32m      4\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m             line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hadoop.conf'"
     ]
    }
   ],
   "source": [
    "# 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "\n",
    "def read_hadoop_config(config_file):\n",
    "    config = {}\n",
    "    with open(config_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                key, value = line.split('=')\n",
    "                config[key.strip()] = value.strip()\n",
    "    return config\n",
    "\n",
    "def display_core_components(config):\n",
    "    if 'fs.defaultFS' in config:\n",
    "        print('File System: {}'.format(config['fs.defaultFS']))\n",
    "    if 'mapreduce.framework.name' in config:\n",
    "        print('MapReduce Framework: {}'.format(config['mapreduce.framework.name']))\n",
    "    if 'dfs.nameservices' in config:\n",
    "        print('NameNode: {}'.format(config['dfs.nameservices']))\n",
    "    if 'yarn.resourcemanager.hostname' in config:\n",
    "        print('ResourceManager: {}'.format(config['yarn.resourcemanager.hostname']))\n",
    "\n",
    "# Example usage\n",
    "config_file = 'hadoop.conf'\n",
    "config = read_hadoop_config(config_file)\n",
    "display_core_components(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def calculate_directory_size(directory_path):\n",
    "    command = ['hdfs', 'dfs', '-du', '-s', directory_path]\n",
    "    output = subprocess.check_output(command).decode('utf-8')\n",
    "    total_size = sum(int(line.split()[0]) for line in output.strip().split('\\n'))\n",
    "    return total_size\n",
    "\n",
    "# Example usage\n",
    "directory_path = '/user/myuser/data'\n",
    "total_size = calculate_directory_size(directory_path)\n",
    "print('Total File Size: {} bytes'.format(total_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the \n",
    "# MapReduce approach.\n",
    "\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "import re\n",
    "\n",
    "def map_function(line):\n",
    "    words = re.findall(r'\\w+', line.lower())\n",
    "    return Counter(words)\n",
    "\n",
    "def reduce_function(counters):\n",
    "    return sum(counters, Counter())\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    return reduce_function(map(map_function, chunk))\n",
    "\n",
    "def get_top_words(filename, top_n):\n",
    "    pool = multiprocessing.Pool()\n",
    "    chunk_size = 1000\n",
    "    with open(filename, 'r') as file:\n",
    "        chunks = [pool.apply_async(process_chunk, args=([file.readline() for _ in range(chunk_size)],)) for _ in range(0, 100000, chunk_size)]\n",
    "        counters = [result.get() for result in chunks]\n",
    "    total_counter = reduce_function(counters)\n",
    "    return total_counter.most_common(top_n)\n",
    "\n",
    "# Example usage\n",
    "filename = 'large_text_file.txt'\n",
    "top_n = 10\n",
    "top_words = get_top_words(filename, top_n)\n",
    "for word, count in top_words:\n",
    "    print('{}: {}'.format(word, count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b506f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's \n",
    "# REST API.\n",
    "\n",
    "import requests\n",
    "\n",
    "def check_namenode_status(nn_url):\n",
    "    response = requests.get(nn_url + '/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus')\n",
    "    data = response.json()\n",
    "    state = data['beans'][0]['State']\n",
    "    live_nodes = data['beans'][0]['NumLiveDataNodes']\n",
    "    dead_nodes = data['beans'][0]['NumDeadDataNodes']\n",
    "    print('NameNode State: {}'.format(state))\n",
    "    print('Live DataNodes: {}'.format(live_nodes))\n",
    "    print('Dead DataNodes: {}'.format(dead_nodes))\n",
    "\n",
    "def check_datanode_status(dn_url):\n",
    "    response = requests.get(dn_url + '/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo')\n",
    "    data = response.json()\n",
    "    state = data['beans'][0]['State']\n",
    "    capacity = data['beans'][0]['Capacity']\n",
    "    used = data['beans'][0]['DfsUsed']\n",
    "    remaining = data['beans'][0]['Remaining']\n",
    "    print('DataNode State: {}'.format(state))\n",
    "    print('Capacity: {} bytes'.format(capacity))\n",
    "    print('Used: {} bytes'.format(used))\n",
    "    print('Remaining: {} bytes'.format(remaining))\n",
    "\n",
    "# Example usage\n",
    "namenode_url = 'http://namenode:50070'\n",
    "datanode_url = 'http://datanode:50075'\n",
    "check_namenode_status(namenode_url)\n",
    "check_datanode_status(datanode_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(path):\n",
    "    command = ['hdfs', 'dfs', '-ls', path]\n",
    "    output = subprocess.check_output(command).decode('utf-8')\n",
    "    lines = output.strip().split('\\n')\n",
    "    files = [line.split()[-1] for line in lines[1:]]\n",
    "    return files\n",
    "\n",
    "# Example usage\n",
    "hdfs_path = '/user/myuser/data'\n",
    "files = list_hdfs_path(hdfs_path)\n",
    "for file in files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the\n",
    "# nodes with the highest and lowest storage capacities.\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_datanode_stats(dn_url):\n",
    "    response = requests.get(dn_url + '/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId')\n",
    "    data = response.json()\n",
    "    stats = data['beans'][0]\n",
    "    return stats\n",
    "\n",
    "def analyze_storage_utilization(datanode_urls):\n",
    "    storage_utilizations = []\n",
    "    for url in datanode_urls:\n",
    "        stats = get_datanode_stats(url)\n",
    "        capacity = stats['Capacity']\n",
    "        used = stats['DfsUsed']\n",
    "        utilization = used / capacity\n",
    "        storage_utilizations.append((url, utilization))\n",
    "    \n",
    "    storage_utilizations.sort(key=lambda x: x[1])\n",
    "    return storage_utilizations\n",
    "\n",
    "# Example usage\n",
    "datanode_urls = ['http://datanode1:50075', 'http://datanode2:50075', 'http://datanode3:50075']\n",
    "storage_utilizations = analyze_storage_utilization(datanode_urls)\n",
    "\n",
    "print('Datanodes with highest storage utilization:')\n",
    "for url, utilization in storage_utilizations[-5:]:\n",
    "    print('{} - Utilization: {:.2%}'.format(url, utilization))\n",
    "\n",
    "print('\\nDatanodes with lowest storage utilization:')\n",
    "for url, utilization in storage_utilizations[:5]:\n",
    "    print('{} - Utilization: {:.7. Here's a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(rm_url, jar_path, main_class, input_path, output_path):\n",
    "    submit_url = rm_url + '/ws/v1/cluster/apps/new-application'\n",
    "    response = requests.post(submit_url)\n",
    "    data = response.json()\n",
    "    app_id = data['application-id']\n",
    "    \n",
    "    submit_job_url = rm_url + '/ws/v1/cluster/apps/' + app_id + '/app'\n",
    "    payload = {\n",
    "        'application-id': app_id,\n",
    "        'application-name': 'MyHadoopJob',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar {} {} {} {}'.format(jar_path, main_class, input_path, output_path)\n",
    "            }\n",
    "        },\n",
    "        'unmanaged-AM': False,\n",
    "        'max-app-attempts': 1\n",
    "    }\n",
    "    \n",
    "    response = requests.put(submit_job_url, json=payload)\n",
    "    if response.status_code == 202:\n",
    "        print('Job submitted successfully.')\n",
    "        return app_id\n",
    "    else:\n",
    "        print('Job submission failed.')\n",
    "        return None\n",
    "\n",
    "def monitor_job_progress(rm_url, app_id):\n",
    "    get_job_url = rm_url + '/ws/v1/cluster/apps/' + app_id\n",
    "    while True:\n",
    "        response = requests.get(get_job_url)\n",
    "        data = response.json()\n",
    "        state = data['app']['state']\n",
    "        progress = data['app']['progress']\n",
    "        print('Job State: {}, Progress: {}%'.format(state, progress))\n",
    "        \n",
    "        if state in ['FINISHED', 'KILLED', 'FAILED']:\n",
    "            break\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_job_output(hdfs_url, output_path):\n",
    "    hdfs_output_url = hdfs_url + output_path\n",
    "    response = requests.get(hdfs_output_url)\n",
    "    if response.status_code == 200:\n",
    "        print('Job output:')\n",
    "        print(response.text)\n",
    "    else:\n",
    "        print('Failed to retrieve job output.')\n",
    "\n",
    "# Example usage\n",
    "rm_url = 'http://resourcemanager:8088'\n",
    "hdfs_url = 'http://namenode:50070'\n",
    "jar_path = 'myjob.jar'\n",
    "main_class = 'com.myjob.Main'\n",
    "input_path = '/input/data.txt'\n",
    "output_path = '/output/result.txt'\n",
    "\n",
    "app_id = submit_hadoop_job(rm_url, jar_path, main_class, input_path, output_path)\n",
    "if app_id:\n",
    "    monitor_job_progress(rm_url, app_id)\n",
    "    retrieve_job_output(hdfs_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress,\n",
    "# and retrieve the final output.\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(rm_url, jar_path, main_class, input_path, output_path, vcores, memory):\n",
    "    submit_url = rm_url + '/ws/v1/cluster/apps/new-application'\n",
    "    response = requests.post(submit_url)\n",
    "    data = response.json()\n",
    "    app_id = data['application-id']\n",
    "    \n",
    "    submit_job_url = rm_url + '/ws/v1/cluster/apps/' + app_id + '/app'\n",
    "    payload = {\n",
    "        'application-id': app_id,\n",
    "        'application-name': 'MyHadoopJob',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar {} {} {} {}'.format(jar_path, main_class, input_path, output_path)\n",
    "            },\n",
    "            'resources': {\n",
    "                'vcores': vcores,\n",
    "                'memory': memory\n",
    "            }\n",
    "        },\n",
    "        'unmanaged-AM': False,\n",
    "        'max-app-attempts': 1\n",
    "    }\n",
    "    \n",
    "    response = requests.put(submit_job_url, json=payload)\n",
    "    if response.status_code == 202:\n",
    "        print('Job submitted successfully.')\n",
    "        return app_id\n",
    "    else:\n",
    "        print('Job submission failed.')\n",
    "        return None\n",
    "\n",
    "def monitor_resource_usage(rm_url, app_id):\n",
    "    get_app_url = rm_url + '/ws/v1/cluster/apps/' + app_id\n",
    "    while True:\n",
    "        response = requests.get(get_app_url)\n",
    "        data = response.json()\n",
    "        state = data['app']['state']\n",
    "        resource_info = data['app']['allocatedResources']\n",
    "        vcores = resource_info['vcoreSeconds']\n",
    "        memory = resource_info['memorySeconds']\n",
    "        print('Job State: {}, Allocated vCores: {}, Allocated Memory: {}'.format(state, vcores, memory))\n",
    "        \n",
    "        if state in ['FINISHED', 'KILLED', 'FAILED']:\n",
    "            break\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "# Example usage\n",
    "rm_url = 'http://resourcemanager:8088'\n",
    "jar_path = 'myjob.jar'\n",
    "main_class = 'com.myjob.Main'\n",
    "input_path = '/input/data.txt'\n",
    "output_path = '/output/result.txt'\n",
    "vcores = 2\n",
    "memory = 2048\n",
    "\n",
    "app_id = submit_hadoop_job(rm_url, jar_path, main_class, input_path, output_path, vcores, memory)\n",
    "if app_id:\n",
    "    monitor_resource_usage(rm_url, app_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements\n",
    "# , and track resource usage during job execution.\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(rm_url, jar_path, main_class, input_path, output_path, vcores, memory):\n",
    "    submit_url = rm_url + '/ws/v1/cluster/apps/new-application'\n",
    "    response = requests.post(submit_url)\n",
    "    data = response.json()\n",
    "    app_id = data['application-id']\n",
    "    \n",
    "    submit_job_url = rm_url + '/ws/v1/cluster/apps/' + app_id + '/app'\n",
    "    payload = {\n",
    "        'application-id': app_id,\n",
    "        'application-name': 'MyHadoopJob',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar {} {} {} {}'.format(jar_path, main_class, input_path, output_path)\n",
    "            },\n",
    "            'resources': {\n",
    "                'vcores': vcores,\n",
    "                'memory': memory\n",
    "            }\n",
    "        },\n",
    "        'unmanaged-AM': False,\n",
    "        'max-app-attempts': 1\n",
    "    }\n",
    "    \n",
    "    response = requests.put(submit_job_url, json=payload)\n",
    "    if response.status_code == 202:\n",
    "        print('Job submitted successfully.')\n",
    "        return app_id\n",
    "    else:\n",
    "        print('Job submission failed.')\n",
    "        return None\n",
    "\n",
    "def monitor_resource_usage(rm_url, app_id):\n",
    "    get_app_url = rm_url + '/ws/v1/cluster/apps/' + app_id\n",
    "    while True:\n",
    "        response = requests.get(get_app_url)\n",
    "        data = response.json()\n",
    "        state = data['app']['state']\n",
    "        resource_info = data['app']['allocatedResources']\n",
    "        vcores = resource_info['vcoreSeconds']\n",
    "        memory = resource_info['memorySeconds']\n",
    "        print('Job State: {}, Allocated vCores: {}, Allocated Memory: {}'.format(state, vcores, memory))\n",
    "        \n",
    "        if state in ['FINISHED', 'KILLED', 'FAILED']:\n",
    "            break\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "# Example usage\n",
    "rm_url = 'http://resourcemanager:8088'\n",
    "jar_path = 'myjob.jar'\n",
    "main_class = 'com.myjob.Main'\n",
    "input_path = '/input/data.txt'\n",
    "output_path = '/output/result.txt'\n",
    "vcores = 2\n",
    "memory = 2048\n",
    "\n",
    "app_id = submit_hadoop_job(rm_url, jar_path, main_class, input_path, output_path, vcores, memory)\n",
    "if app_id:\n",
    "    monitor_resource_usage(rm_url, app_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing \n",
    "# the impact on overall job execution time.\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_mapreduce_job(input_path, output_path, split_size):\n",
    "    start_time = time.time()\n",
    "    command = ['hadoop', 'jar', 'myjob.jar', 'com.myjob.Main', '-D', 'mapreduce.input.fileinputformat.split.minsize=' + split_size, input_path, output_path]\n",
    "    subprocess.run(command)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return execution_time\n",
    "\n",
    "# Example usage\n",
    "input_path = '/input/data.txt'\n",
    "output_path = '/output/result.txt'\n",
    "split_sizes = ['64m', '128m', '256m']\n",
    "\n",
    "for split_size in split_sizes:\n",
    "    execution_time = run_mapreduce_job(input_path, output_path, split_size)\n",
    "    print('Split Size: {}, Execution Time: {} seconds'.format(split_size,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcff959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
